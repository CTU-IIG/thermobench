\documentclass[a4paper,UKenglish]{oasics}
%This is a template for producing OASIcs articles.
%See oasics-manual.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"

\usepackage{microtype}%if unwanted, comment out or use option "draft"

\usepackage{booktabs}

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plain}% the recommended bibstyle


\newcommand{\todo}[1]{{\emph{TODO: #1}}}
\newcommand{\martin}[1]{{\color{blue} Martin: #1}}
\newcommand{\abcdef}[1]{{\color{red} Author2: #1}}

\newcommand{\code}[1]{{\small{\texttt{#1}}}}

% fatter TT font
\renewcommand*\ttdefault{txtt}

% Author macros::begin %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{TACLeBench -- Title is TBD\footnote{This work was partially supported by COST Action IC1202 Timing Analysis on Code-Level (TACLe).}}
%\titlerunning{TACLeBench} %optional, in case that the title is too long; the running title should fit into the top page column

\author[1]{Heiko Falk}
\author[2]{Wolfgang Puffitsch}
\author[3]{Bj{\"o}rn Lisper}
\author[2]{Martin Schoeberl}
\author[2]{Add yourself in alphabetical order, but keep Heiko first.}
\affil[1]{Dummy University Computing Laboratory\\
  Address, Country\\
  \texttt{open@dummyuni.org}}
\affil[2]{Technical University of Denmark, Department of Applied Mathematics and Computer Science, Denmark\\
  \texttt{\{wopu, masca\}@dtu.dk}}
\affil[3]{M{\"a}lardalen, University, School of Innovation, Design, and Engineering, Sweden\\
  \texttt{bjorn.lisper@mdh.se}}
  \authorrunning{H.~Falk et al.}%mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et. al.'

\Copyright{Heiko Falk et al.}%mandatory. OASIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\subjclass{Dummy classification -- please refer to \url{http://www.acm.org/about/class/ccs98-html}}% mandatory: Please choose ACM 1998 classifications from http://www.acm.org/about/class/ccs98-html . E.g., cite as "F.1.1 Models of Computation". 
\keywords{Dummy keywords -- please provide 1--5 keywords}% mandatory: Please provide 1-5 keywords
% Author macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\serieslogo{}%please provide filename (without suffix)
\volumeinfo%(easychair interface)
  {Martin Schoeberl}% editors
  {2}% number of editors: 1, 2, ....
  {Conference/workshop/symposium title on which this volume is based on}% event
  {1}% volume
  {1}% issue
  {1}% starting page number
\EventShortName{}
\DOI{10.4230/OASIcs.xxx.yyy.p}% to be completed by the volume editor
% Editor-only macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

\begin{abstract}
\todo{Martin will work on this.}

We need benchmarks that fit the WCET and embedded community needs,


 \end{abstract}

\section{Notes}

\todo{Just internal notes to collect bullet points on ideas.}

\begin{itemize}
\item On issue with original benchmarks optimized away - we can now check with Patmos easily
\item Working on getting the license correct - issue with code from unknown source
\item Useful in general for embedded systems/barebone systems where less libraries are available
\item Usage of the benchmarks (version, no subsetting, no source change)
\item How to contribute
\end{itemize}

\section{Introduction}
\label{sec:intro}

\todo{Martin will work on this.}

     Good, realistic benchmark suites are essential for the evaluation and comparison of code-level timing analysis techniques. Existing benchmarking suites for CLTA do not cover multi-core software. TACLeBench provides a freely available and comprehensive benchmark suite for timing analysis, featuring complex multi-core benchmarks in the near future. TACLeBench will be continuously extended by novel benchmarks, especially by parallel multi-task/multi-core benchmarks. The overall goal is to establish TACLeBench as the standard benchmarking suite for timing analysis worldwide.

    TACLeBench is a collection of currently 102 benchmark programs from several different research groups and tool vendors around the world. These benchmarks are provided as ANSI-C 99 source codes. The source codes are 100\% self-contained; no dependencies to system-specific header files via \code{\#include} directives exist, eventually used functions from math libraries are also provided in the form of C source code. Furthermore, almost all benchmarks are processor-independent and can be compiled and evaluated for any kind of target processor. The only exception is PapaBench which uses architecture-dependent I/O addresses and which currently supports Atmel AVR processors.

    Since TACLeBench addresses the needs imposed by timing analysis tools, all benchmarks are completely annotated with flow facts. These flow facts are directly incorporated into the ANSI-C source codes using Pragmas at a high level of abstraction. For example, the following source code snippet states that the shown loop iterates between 50 and 100 times, depending on the data dependency via variable \code{maxIter}:

\begin{verbatim}
      _Pragma( "loopbound min 50 max 100" )

      for ( i = 1; i <= maxIter; i++ )

        Array[i] = i * fact * KNOWN_VALUE;

\end{verbatim}
    You can find a complete description of the used flow fact language in the download section of this website.

    If you would like to share your very own benchmarks with us, feel free to contact Heiko Falk (Heiko (dot) Falk (at) tuhh (dot) de) in order to have your source codes included in TACLeBench.

\todo{A brief introduction what the paper is about. It shall include briefly the
main contributions and findings. The contributions can be bullet listed.}

This paper... \todo{purpose statement, latest in 4th paragraph}

\todo{We are aiming on self-contained benchmarks, that do not need any
operating system services, including file IO. Therefore,...}

\todo{Talk about the TACLe COST action and the subgroup around the benchmark
collection. And plans how to further develop and maintain the collection.}

\todo{Talk about usage of the benchmarks: WCET tools, compiler, hardware
architecture,...}

The first version of TACLeBench (version 1.0, available
from\footnote{\url{http://www.tacle.eu/index.php/activities/taclebench}}), which was produced
by Heiko Falk, was a collection of 102 benchmark programs from several different
research groups. We keep this first version tagged with ``V1.0'' in the public
GitHub repository.\footnote{\url{https://github.com/tacle/tacle-bench}}
The version described in this paper is version 2.0 and tagged as such in the
repository.

The contributions of this paper are: (1) ... (2) ...

This paper is organized in N sections: The following section presents related work.
Section~\ref{sec:related} presents related work.
Section~\ref{sec:collect} presents the benchmark collection, its classification, and the updates
to make them useful.
Section~\ref{sec:eval} evaluates the benchmark collection on...
Section~\ref{sec:conclusion} concludes.

\section{Related Work}
\label{sec:related}

\todo{Show that you know the field. All related work shall be put
into context or contrast to our current work.}

The M{\"a}lardalen WCET benchmarks is the first collection of programs especially
intended for benchmarking WCET analysis tools, with a focus on program flow analysis~\cite{wcet:bench:2012}.
This collection of C programs was an attempt to organise a number of codes, which had up to then been floating around
in the research community, into a repository with a more systematic treatment of meta-data and easy web access.
It was collected from several sources in 2005, and has since then been
used in many WCET research projects as well as for the WCET Tool Challenge 2006~\cite{Gustafsson:ISOLA2006}.
A subset of the M{\"a}lardalen benchmarks has even been translated to
Java~\cite{jop:volta:rtas2008}. Most benchmarks are relatively small, except
two C programs that have been generated from tools. The benchmarks also
contain all input data. This effectively turns them into single-path programs, which
makes them less suitable for evaluating tools that can handle multi-path codes. To remedy that,
some of the benchmarks are equipped with suggested input ranges that override the fixed
input data in the code. The user is however then responsible to use the proper settings of the tool,
or to create an appropriate test harness, such that the programs are analysed with the specified input ranges
rather than the fixed input data. Another consequence of the fact that the
input data is fixed, and that some programs do not provide
any return value, is that good compilers with optimization turned on can optimize
most of the code away. We include most of the benchmarks from the
 M{\"a}lardalen WCET benchmark suite in TACLeBench. However, to prohibit the unwanted compiler optimizations we changed
 the way input data is represented in variables (made them \code{volatile}),
 and made the return of \code{main} dependent on the benchmark calculation.
 Furthermore, we dropped benchmarks where the licensing terms are unknown
 or even disallow distributing the source.

EEMBC

MiBench~\cite{MiBench} We include some of the MiBench benchmarks, especially
those where it was possible to include the input data with the C source.

Debie~\cite{debie}

PapaBench~\cite{papabench}

General purpose benchmarking, with a focus on open source and embedded systems.

JemBench.

\section{The Benchmark Collection}
\label{sec:collect}


\subsection{Benchmark Sources}

\todo{what did we collect. How many indirections in collections have there be?
E.g., adpcm.c: (1) SOURCE : C Algorithms for Real-Time DSP by P. M. Embree,
(2) SNU-RT Benchmark Suite, (3) MDH, and (4) TACLeBench. How have
benchmarks  changed? Legal changes?}

\subsection{Classification}

\todo{Talk about kernel, medium size, and application benchmarks and their usage.}
\begin{itemize}
\item Static analysis
\item Measurement based analysis
\item Schedulability analysis using taskset generator
\end{itemize}

\todo{The following is a verbatim copy from the TACLe website.
Needs to be updated and put into a nice table, like the following template.}

\begin{table}
  \centering
  \caption{TECLeBench benchmarks}
  \label{tab:bench}
  \begin{tabular}{llrl}
    \toprule
    Name & Description & Code Size (LoC) & Origin \\
    \midrule
    adpcm\_decoder & ADPCM decoder & 406 & ??? \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{verbatim}
 Sequential Benchmarks:
Name 	Description 	Code Size (LoC) 1 	Origin
adpcm_decoder 	ADPCM decoder 	406 	MRTC
adpcm_encoder 	ADPCM encoder 	434 	MRTC
adpcm_g721_board_test 	ADPCM transcoder 	661 	DSPStone
adpcm_g721_verify 	ADPCM verification 	673 	DSPStone
ammunition 	C compiler arithmetic stress test 	2508 	misc
anagram 	Word anagram computation 	440 	misc
audiobeam 	Audio beam former 	985 	StreamIt
basicmath_small 	Basic mathematical calculations (cubic function solving, integer square roots, angle conversion) 	1001 	MiBench
binarysearch 	Binary search of 15 integers 	35 	MRTC
bitcount 	Couting number of bits in an integer array 	202 	MiBench
bitonic 	Bitonic sorting network 	56 	StreamIt
bsort100 	Bubblesort program 	54 	MRTC
cjpeg_jpeg6b_transupp 	JPEG image transcoding routines 	1599 	MediaBench
cjpeg_jpeg6b_wrbmp 	JPEG image bitmap writing code 	1296 	MediaBench
codecs_codhuff 	Huffman encoding with a file source to compress 	373 	misc
codecs_codrle1 	RLE type 1 encoding with a file source to compress 	110 	misc
codecs_dcodhuff 	Huffman decoding with a file source to decompress 	221 	misc
codecs_dcodrle1 	RLE type 1 decoding with a file source to decompress 	60 	misc
complex_multiply_fixed 	Multiplication of complex numbers 	22 	DSPStone
complex_multiply_float 	Multiplication of complex numbers 	22 	DSPStone
complex_update_fixed 	Multiply-add with complex numbers 	26 	DSPStone
complex_update_float 	Multiply-add with complex numbers 	28 	DSPStone
compressdata 	Data compression program adopted from SPEC95 	203 	MRTC
convolution_fixed 	Convolution filter 	27 	DSPStone
convolution_float 	Convolution filter 	27 	DSPStone
countnegative 	Counts non-negative numbers in a matrix 	73 	MRTC
cover 	Artificial code with lots of different control flow paths 	231 	MRTC
crc 	CRC computation on 40 bytes of data 	68 	MRTC
dijkstra 	All pairs shortest path 	227 	MiBench
dot_product_fixed 	Dot product of vectors 	22 	DSPStone
dot_product_float 	Dot product of vectors 	24 	DSPStone
duff 	Duff's device 	44 	MRTC
edn 	FIR filter calculations 	204 	MRTC
epic 	Efficient pyramid image coder 	994 	MediaBench
expint 	Series expansion for computing an exponential integral function. 	62 	MRTC
fac 	Faculty function 	21 	MRTC
fdct 	Fast discrete cosine transform 	143 	MRTC
fft_16_7 	16-point FFT, 7 bits per twiddle 	149 	DSPStone
fft_16_13 	16-point FFT, 13 bits per twiddle 	156 	DSPStone
fft_1024_7 	1024-point FFT, 7 bits per twiddle 	282 	DSPStone
fft_1024_13 	1024-point FFT, 13 bits per twiddle 	311 	DSPStone
fft1 	1024-point FFT using the Cooly-Turkey algorithm 	140 	MRTC
fibcall 	Simple iterative Fibonacci calculation 	22 	MRTC
filterbank 	Filter bank for multirate signal processing 	75 	StreamIt
fir 	FIR filter over a sample of 700 items 	225 	MRTC
fir2dim_fixed 	2-dimensional FIR filter convolution 	81 	DSPStone
fir2dim_float 	2-dimensional FIR filter convolution 	81 	DSPStone
fir_fixed 	FIR filter 	40 	DSPStone
fir_float 	FIR filter 	40 	DSPStone
fmref 	Software FM radio with equalizer 	680 	StreamIt
g721_encode 	CCITT G.721 encoder 	901 	misc
g723_encode 	CCITT G.723 encoder 	898 	misc
gsm 	Complete GSM 06.10 provisional standard codec 	2382 	MediaBench
gsm_decode 	GSM 06.10 provisional standard decoder 	1368 	MediaBench
gsm_encode 	GSM 06.10 provisional standard encoder 	1940 	MediaBench
h264dec_ldecode_block 	H.264 block decoding functions 	1574 	MediaBench
h264dec_ldecode_macroblock 	H.264 macroblock decoding 	1144 	MediaBench
iir_biquad_N_sections_fixed 	Biquad IIR 4 sections filter 	42 	DSPStone
iir_biquad_N_sections_float 	Biquad IIR 4 sections filter 	43 	DSPStone
iir_biquad_one_section_fixed 	Biquad IIR 1 section filter 	25 	DSPStone
iir_biquad_one_section_float 	Biquad IIR 1 section filter 	25 	DSPStone
insertsort 	Insertion sort 	61 	MRTC
janne_complex 	Artificial code where the inner loop's max. iterations depend on the outer loop 	61 	MRTC
jfdctint 	Discrete-cosine transformation on a 8x8 pixel block 	221 	MRTC
lcdnum 	Read ten values, output half to LCD 	62 	MRTC
lms 	LMS adaptive signal enhancement 	160 	MRTC
lms_fixed 	Least-Mean-Square filter 	44 	DSPStone
lms_float 	Least-Mean-Square filter 	44 	DSPStone
ludcmp 	LU decomposition 	86 	MRTC
matmult 	20x20 matrix multiplication 	57 	MRTC
matrix1x3_fixed 	1x3 matrix multiplication 	23 	DSPStone
matrix1x3_float 	1x3 matrix multiplication 	41 	DSPStone
matrix1_fixed 	Generic matrix multiplication 	48 	DSPStone
matrix1_float 	Generic matrix multiplication 	49 	DSPStone
md5 	Message digest algorithm 	336 	NetBench
minver 	Floating point matrix inversion 	159 	MRTC
mpeg2 	MPEG2 motion estimation 	1533 	MediaBench
n_complex_updates_fixed 	Multiply-add with vectors of complex numbers 	38 	DSPStone
n_complex_updates_float 	Multiply-add with vectors of complex numbers 	38 	DSPStone
n_real_updates_fixed 	Multiply-add with vectors of real numbers 	29 	DSPStone
n_real_updates_float 	Multiply-add with vectors of real numbers 	29 	DSPStone
ndes 	Complex embedded code 	407 	MRTC
petrinet 	Petri net simulation 	500 	MRTC
pm 	Pattern match kernel 	558 	misc
prime 	Prime number test 	34 	MRTC
qsort 	Quick sort of string arrays 	2716 	MiBench
qsort-exam 	Non-recursive quicksort 	84 	MRTC
qurt 	Root computation of quadratic equations 	88 	MRTC
real_update_fixed 	Multiply-add with real numbers 	21 	DSPStone
real_update_float 	Multiply-add with real numbers 	24 	DSPStone
recursion 	Artificial recursive code 	16 	MRTC
rijndael_decoder 	Rijndael AES decryption 	3043 	MiBench
rijndael_encoder 	Rijndael AES encryption 	1024 	MiBench
select 	Select the Nth largest number in a floating point array 	62 	MRTC
sha 	NIST secure hash algorithm 	509 	MiBench
sqrt 	Square root function implemented by Taylor series 	45 	MRTC
st 	Statistics program (sum, mean, variance, std. deviation, correlation) 	106 	MRTC
startup_fixed 	V32 modem startup code 	99 	DSPStone
statemate 	Statechart simulation of a car window lift control 	1053 	MRTC
susan 	MR image recognition algorithm 	1984 	MiBench
test3 	Artificial WCET analysis stress test 	3985 	misc
Parallel Benchmarks:
Name 	Description 	#Tasks 	Code Size (LoC) 1 	Origin
Debie 	DEBIE-1 instrument observing micro-meteoroids and small space debris 	8 	6094 	Tidorum Ltd
PapaBench 	UAV autopilot and fly-by-wire software 	10 	6644 	Paparazzi
\end{verbatim}


\subsection{Issues with the Original Sources}


\todo{Show the issues with code snippets}

\begin{itemize}
\item Compiler optimization
\item byte order
\item low-level access to device registers
\item Some benchmarks, which are available for download in the Internet have a license that
does not allow open-source distribution. We removed those benchmarks from the original source.
\end{itemize}

\subsection{Benchmark Changes}



\begin{itemize}
\item unique names for functions and global variables
\item Sometimes movements of stack allocated variables into global
\item splitting of initialization and computation code into two functions
\item Loop bounds added
\end{itemize}


\subsection{Usage Recommendations}

\todo{Mmh shall we have this?}

\todo{can we include a paragraph about the taskset generator for schedulability analysis}

\begin{itemize}
\item Use a stable version. At the time of this writing we should have V 2.0
\item Do not edit the benchmarks
\item Use all benchmarks in your evaluation. Don't introduce a bias in your evaluation by
\emph{selecting the most representative benchmarks} for your improvement
\end{itemize}

\subsection{Licenses}

An issue we encountered with several benchmarks was that the original
source did not include any licensing information. In absence of such
information, we had to assume that the copyright holder reserves all
rights. Wherever necessary, we contacted the copyright holders to
obtain the right to use, modify, and redistribute the benchmarks. In a
small number of cases we however discovered that the code was in fact
under a license that made the benchmark unusable; the respective
benchmarks were consequently dropped from the TACLeBench benchmark
suite. All benchmarks in the benchmark suite now contain licensing
information, such that future developments do not require tracking
down the original authors of the benchmark.

\section{Evaluation}
\label{sec:eval}

\todo{Computer engineering is a constructive science. We build stuff and we measure.
Therefore, there shall always be an evaluation section.}

Possible Options for the Evaluation:

\begin{itemize}
\item Compile for an embedded processor and measure execution time
\item Compile for Patmos and use pasim for execution cycles
\item Show how compiler optimizations optimized away the original code and how we fixed this
\item Complexity numbers with bar charts on execution time and maybe tools
\item Use some WCET tools
\end{itemize}


\section{Conclusion}
\label{sec:conclusion}

\todo{Rephrase what this paper is about and list the main contributions and results.}

\subparagraph*{Acknowledgements}

\todo{TACLe COST action phrase}

We want to thank \dots for helping in restructuring the benchmarks.
We want to thank Bendikt Huber for porting the Lift benchmark from Java to C.

\todo{Ack Niclas for contributing DEBIE in open-source.}

\appendix
\section{Source Access and Compiling the Benchmarks}


\bibliography{taclebench}


\end{document}
