\documentclass[a4paper,UKenglish]{oasics}
%This is a template for producing OASIcs articles.
%See oasics-manual.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"

\usepackage{microtype}%if unwanted, comment out or use option "draft"

\usepackage{booktabs}

\usepackage{longtable}
\usepackage{array}
\usepackage{tikz}
\usepackage{pgfplots}

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plain}% the recommended bibstyle


\newcommand{\todo}[1]{{\emph{TODO: #1}}}
\newcommand{\martin}[1]{{\color{blue} Martin: #1}}
\newcommand{\abcdef}[1]{{\color{red} Author2: #1}}

\newcommand{\code}[1]{{\small{\texttt{#1}}}}

% fatter TT font
\renewcommand*\ttdefault{txtt}

% Author macros::begin %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{TACLeBench -- Title is TBD\footnote{This work was partially supported by COST Action IC1202 Timing Analysis on Code-Level (TACLe).}}
%\titlerunning{TACLeBench} %optional, in case that the title is too long; the running title should fit into the top page column

% Heiko first, rest in alphabetical order
\author[1]{Heiko Falk}
<<<<<<< Updated upstream
\author[2]{Sebastian Altmeyer}
\author[3]{Bj{\"o}rn Lisper}
\author[4]{Wolfgang Puffitsch}
\author[5]{Christine Rochange}
\author[4]{Martin Schoeberl}
\author[4]{Rasmus Bo S{\o}rensen}
\author[6]{Peter W{\"a}gemann}
\affil[1]{Dummy University Computing Laboratory\\
  Address, Country\\
  \texttt{open@dummyuni.org}}
\affil[2]{University of Amsterdam, The Netherlands\\
  \texttt{altmeyer@uva.nl}}
\affil[3]{M{\"a}lardalen, University, School of Innovation, Design, and Engineering, Sweden\\
  \texttt{bjorn.lisper@mdh.se}}
\affil[4]{Technical University of Denmark, Department of Applied Mathematics and Computer Science, Denmark\\
  \texttt{\{wopu, masca,rboso\}@dtu.dk}}
\affil[5]{University of Toulouse, France\\
  \texttt{rochange@irit.fr}}
\affil[6]{Friedrich-Alexander University Erlangen-NÃ¼rnberg, Germany\\
  \texttt{waegemann@cs.fau.de}}
\affil[7]{University of Antwerp, iMinds, Belgium\\
  \texttt{peter.hellinckx@uantwerpen.be}}
  \authorrunning{H.~Falk et al.}%mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et. al.'

\Copyright{Heiko Falk et al.}%mandatory. OASIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\subjclass{Dummy classification -- please refer to \url{http://www.acm.org/about/class/ccs98-html}}% mandatory: Please choose ACM 1998 classifications from http://www.acm.org/about/class/ccs98-html . E.g., cite as "F.1.1 Models of Computation". 
\keywords{Dummy keywords -- please provide 1--5 keywords}% mandatory: Please provide 1-5 keywords
% Author macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\serieslogo{}%please provide filename (without suffix)
\volumeinfo%(easychair interface)
  {Martin Schoeberl}% editors
  {2}% number of editors: 1, 2, ....
  {Conference/workshop/symposium title on which this volume is based on}% event
  {1}% volume
  {1}% issue
  {1}% starting page number
\EventShortName{}
\DOI{10.4230/OASIcs.xxx.yyy.p}% to be completed by the volume editor
% Editor-only macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

\begin{abstract}
\todo{Martin will work on this.}

We need benchmarks that fit the WCET and embedded community needs,


 \end{abstract}

\section{Notes}

\todo{Just internal notes to collect bullet points on ideas.}

\begin{itemize}
\item On issue with original benchmarks optimized away - we can now check with Patmos easily
\item Working on getting the license correct - issue with code from unknown source
\item Useful in general for embedded systems/barebone systems where less libraries are available
\item Usage of the benchmarks (version, no subsetting, no source change)
\item How to contribute
\end{itemize}

\section{Introduction}
\label{sec:intro}

\todo{Martin will work on this.}

     Good, realistic benchmark suites are essential for the evaluation and comparison of code-level timing analysis techniques. Existing benchmarking suites for CLTA do not cover multi-core software. TACLeBench provides a freely available and comprehensive benchmark suite for timing analysis, featuring complex multi-core benchmarks in the near future. TACLeBench will be continuously extended by novel benchmarks, especially by parallel multi-task/multi-core benchmarks. The overall goal is to establish TACLeBench as the standard benchmarking suite for timing analysis worldwide.

    TACLeBench is a collection of currently 102 benchmark programs from several different research groups and tool vendors around the world. These benchmarks are provided as ANSI-C 99 source codes. The source codes are 100\% self-contained; no dependencies to system-specific header files via \code{\#include} directives exist, eventually used functions from math libraries are also provided in the form of C source code. Furthermore, almost all benchmarks are processor-independent and can be compiled and evaluated for any kind of target processor. The only exception is PapaBench which uses architecture-dependent I/O addresses and which currently supports Atmel AVR processors.

    Since TACLeBench addresses the needs imposed by timing analysis tools, all benchmarks are completely annotated with flow facts. These flow facts are directly incorporated into the ANSI-C source codes using Pragmas at a high level of abstraction. For example, the following source code snippet states that the shown loop iterates between 50 and 100 times, depending on the data dependency via variable \code{maxIter}:

\begin{verbatim}
      _Pragma( "loopbound min 50 max 100" )

      for ( i = 1; i <= maxIter; i++ )

        Array[i] = i * fact * KNOWN_VALUE;

\end{verbatim}
    You can find a complete description of the used flow fact language in the download section of this website.

    If you would like to share your very own benchmarks with us, feel free to contact Heiko Falk (Heiko (dot) Falk (at) tuhh (dot) de) in order to have your source codes included in TACLeBench.

\todo{A brief introduction what the paper is about. It shall include briefly the
main contributions and findings. The contributions can be bullet listed.}

This paper... \todo{purpose statement, latest in 4th paragraph}

\todo{We are aiming on self-contained benchmarks, that do not need any
operating system services, including file IO. Therefore,...}

\todo{Talk about the TACLe COST action and the subgroup around the benchmark
collection. And plans how to further develop and maintain the collection.}

\todo{Talk about usage of the benchmarks: WCET tools, compiler, hardware
architecture,...}

The first version of TACLeBench (version 1.0, available
from\footnote{\url{http://www.tacle.eu/index.php/activities/taclebench}}), which was produced
by Heiko Falk, was a collection of 102 benchmark programs from several different
research groups. We keep this first version tagged with ``V1.0'' in the public
GitHub repository.\footnote{\url{https://github.com/tacle/tacle-bench}}
The version described in this paper is version 2.0 and tagged as such in the
repository.

The contributions of this paper are: (1) ... (2) ...

This paper is organized in N sections: The following section presents related work.
Section~\ref{sec:related} presents related work.
Section~\ref{sec:collect} presents the benchmark collection, its classification, and the updates
to make them useful.
Section~\ref{sec:eval} evaluates the benchmark collection on...
Section~\ref{sec:conclusion} concludes.

\section{Related Work}
\label{sec:related}

\todo{Show that you know the field. All related work shall be put
into context or contrast to our current work.}

The M{\"a}lardalen WCET benchmarks is the first collection of programs especially
intended for benchmarking WCET analysis tools, with a focus on program flow analysis~\cite{wcet:bench:2012}.
This collection of C programs was an attempt to organise a number of codes, which had up to then been floating around
in the research community, into a repository with a more systematic treatment of meta-data and easy web access.
It was collected from several sources in 2005, and has since then been
used in many WCET research projects as well as for the WCET Tool Challenge 2006~\cite{Gustafsson:ISOLA2006}.
A subset of the M{\"a}lardalen benchmarks has even been translated to
Java~\cite{jop:volta:rtas2008}. Most benchmarks are relatively small, except
two C programs that have been generated from tools. The benchmarks also
contain all input data. This effectively turns them into single-path programs, which
makes them less suitable for evaluating tools that can handle multi-path codes. To remedy that,
some of the benchmarks are equipped with suggested input ranges that override the fixed
input data in the code. The user is however then responsible to use the proper settings of the tool,
or to create an appropriate test harness, such that the programs are analysed with the specified input ranges
rather than the fixed input data. Another consequence of the fact that the
input data is fixed, and that some programs do not provide
any return value, is that good compilers with optimization turned on can optimize
most of the code away. We include most of the benchmarks from the
 M{\"a}lardalen WCET benchmark suite in TACLeBench. However, to prohibit the unwanted compiler optimizations we changed
 the way input data is represented in variables (made them \code{volatile}),
 and made the return of \code{main} dependent on the benchmark calculation.
 Furthermore, we dropped benchmarks where the licensing terms are unknown
 or even disallow distributing the source.


MiBench~\cite{MiBench} We include some of the MiBench benchmarks, especially
those where it was possible to include the input data with the C source.

Debie~\cite{debie}

PapaBench~\cite{papabench} has been derived from  Paparazzi\footnote{\url{https://wiki.paparazziuav.org/}}, 
an open-source hardware and software project for the design of unmaned aerial vehicles.  PapaBench includes 
two software components of this project, that run on separate processors communicating through an SPI link:
the \textit{fly-by-wire} part controls the flight (engine and flaps control, stabilization, radio-communication 
with the ground, support for infrared sensors, etc.) while the \textit{autopilot} part controls the GPS and 
executes the flight plan (which is decided offline). Both software parts  cumulate 13 tasks that are subject
to precedence constraints and 6 interrupt service routines.

The Embedded Microprocessor Benchmark Consortium (EEMBC)~\cite{eembc} provides their own benchmark suite dedicated to the evaluation of 
the performance of embedded hardware and embedded software.
The benchmarks are divided into subset according to the target domain, e.g., the automotive domain, phones and tables, but also big data and cloud computing.
To improve comparability between different processors and systems, the consortium provides a sophisticated test-harness which allows to derive certifiable scores.
The test-harness, being a clear advantage in terms of comparability and certifiability, constitutes a hindrance in terms of portability and usability.
Whereas the TACLeBench has been designed to ease portability and to allow the immediate use of the benchmark with a large variety of tools and platforms, the EEMBC benchmarks are not stand-alone executable without the test-harness.
Furthermore, in stark contrast to the aforementioned benchmark suites, and especially to TACLeBench, the EEMBC benchmarks are not published under an open-source license. 
Instead, the benchmarks are behind a pay-wall, even for purely academic research.


JemBench.

\section{The Benchmark Collection}
\label{sec:collect}


\subsection{Benchmark Sources}

\todo{what did we collect. How many indirections in collections have there be?
E.g., adpcm.c: (1) SOURCE : C Algorithms for Real-Time DSP by P. M. Embree,
(2) SNU-RT Benchmark Suite, (3) MDH, and (4) TACLeBench. How have
benchmarks  changed? Legal changes?}

\subsection{Classification}

\todo{Talk about kernel, medium size, and application benchmarks and their usage.}
\begin{itemize}
\item Static analysis
\item Measurement based analysis
\item Hybrid analysis: The hybrid model splits the code of a set of tasks into so-
called basic blocks. Each basic block contains one path trace of instructions with a single set of inputs and a single set of outputs.  The  size  of  a  basic  block  varies  between  the  largest possible trace meeting the constraints of a single set of inputs and outputs over a single memory block, i.e. a unit of memory that is replaced by a cache replacement algorithm, to a single program instruction.
The  challenge  of  this  two  layer  hybrid  approach  is  tackling the computational complexity problems within the static analysis  layer  and  the  accuracy  within  the  measurements based  layer.  In  other  words,  a  proper  balance  needs  to  be found between the two layers of the hybrid model. The TACLeBench is used in the COBRA-HPA (COde Behaviour fRAmework-Hybrid Program Analyser) framework which facilitates evaluation of the different approaches using different block sizes.
\item Schedulability analysis using TACLeBench is facilitated by the the COBRA-TG (Taskset Generator). Different scheduling methodologies can be analysed in a reproducible way using generated tasksets based on specific application descriptions. It replaces the limited set of universal scheduling benchmarks and the wide variety of non-reproducible benchmark application with a methodology that enables for an unlimited but reproducible set of tasksets.

Based on taskset descriptions 
\end{itemize}

\todo{The following is a verbatim copy from the TACLe website.
Needs to be updated and put into a nice table, like the following template.}

\begin{center}
\begin{longtable}{lp{5cm}rr} 
 \caption{TACLeBench benchmarks}  \label{tab:bench} \\
  \toprule Name & Description & Code Size & Origin\\
   & &  (LoC) & \\ \midrule 
  \endfirsthead

  \multicolumn{4}{c}%
  {{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\
  \toprule Name & Description & Code Size & Origin\\
   & &  (LoC) & \\ \midrule 
  \endhead

  \midrule  \multicolumn{4}{r}{Continued on next page} \\ \bottomrule
  \endfoot

  \bottomrule
  \endlastfoot

    adpcm\_dec & ADPCM decoder & 398 & SNU-RT \\
    adpcm\_enc &  ADPCM encoder  & 410  & SNU-RT \\
    ammunition & C compiler arithmetic stress test  & 2448 & DINO \\
    anagram &  Word anagram computation & 2772  & misc \\
    audiobeam &  Audio beam former  & 6653  & StreamIt \\
    %basicmath\_small &  Basic mathematical calculations (cubic function solving, integer square roots, angle conversion) & 1001 & MiBench \\
    binarysearch & Binary search of 15 integers & 67 & MRTC \\
    bitcount & Couting number of bits in an integer array & 230  & snippest \\
    bitonic &  Bitonic sorting network  & 73 & MiBench \\
    bsort & Bubblesort program & 60 & MRTC \\
    cjpeg\_transupp &  JPEG image transcoding routines  & 673 & MediaBench \\
    cjpeg\_wrbmp & JPEG image bitmap writing code & 965 & IJG \\
    complex\_updates &  Multiply-add with vectors of complex numbers & 59 & DSPStone \\
    countnegative &  Counts non-negative numbers in a matrix  & 78 & MRTC \\
    cover &  Artificial code with lots of different control flow paths  & 636  & MRTC \\
    crc &  CRC computation on 40 bytes of data  & 81 & SNU-RT \\
    dijkstra & All pairs shortest path  & 247  & MiBench \\
    duff & Duff's device  & 67 & MRTC \\
    epic & Efficient pyramid image coder  & 979  & MediaBench \\

    fac &  Factorial function & 40 & MRTC \\
    fft &  1024-point FFT, 13 bits per twiddle  & 488  & DSPStone \\
    filterbank & Filter bank for multirate signal processing  & 89 & StreamIt \\
    fir2dim &  2-dimensional FIR filter convolution & 119 & DSPStone \\
    fmref &  Software FM radio with equalizer & 680  & StreamIt \\
    g723\_enc &  CCITT G.723 encoder  & 546  & SUN Microsystems \\
    gsm\_dec & GSM provisional standard decoder & 1426 & MediaBench \\
    gsm\_enc & GSM provisional standard encoder & 1951 & MediaBench \\
    h264\_dec &  H.264 block decoding functions & 1053 & MediaBench \\
    huff\_dec &  Huffman decoding with a file source to decompress  & 242  & misc \\
    huff\_enc & Huffman encoding with a file source to compress  & 386  & misc \\
    iir &  Biquad IIR 4 sections filter & 67 & DSPStone \\
    insertsort & Insertion sort & 70 & MRTC \\
    jfdctint & Discrete-cosine transformation on a 8x8 pixel block  & 150  & MRTC \\
    lms &  LMS adaptive signal enhancement  & 160  & MRTC \\
    ludcmp & LU decomposition & 111 & MRTC \\
    matrix1 &  Generic matrix multiplication  & 63 & DSPStone \\
    md5 &  Message digest algorithm & 355  & NetBench \\
    minver & Floating point matrix inversion  & 176  & MRTC \\
    mpeg2 &  MPEG2 motion estimation  & 12601 & MediaBench \\
    ndes & Complex embedded code  & 299  & MRTC \\

    petrinet & Petri net simulation & 500  & MRTC \\
    pm & Pattern match kernel & 1571  & misc \\
    prime &  Prime number test  & 67 & MRTC \\

    quicksort &  Quick sort of string arrays  & 1613 & MiBench \\
    recursion &  Artificial recursive code  & 36 & MRTC \\
    rijndael\_dec & Rijndael AES decryption  & 2922 & MiBench \\
    rijndael\_enc & Rijndael AES encryption  & 2749 & MiBench \\
    select & Select the Nth largest number in a floating point array  & 102 & MRTC \\
    sha &  NIST secure hash algorithm & 2049  & MiBench \\
    st & Statistics program (sum, mean, variance, std. deviation, correlation)  & 127  & MRTC \\
    statemate &  Statechart simulation of a car window lift control & 1107 & MRTC \\
    susan &  MR image recognition algorithm & 8782 & MiBench \\
    test3 &  Artificial WCET analysis stress test & 4297 & misc \\
\end{longtable}
\end{center}


\begin{verbatim}
Parallel Benchmarks:
Name 	Description 	#Tasks 	Code Size (LoC) 1 	Origin
Debie 	DEBIE-1 instrument observing micro-meteoroids and small space debris 	8 	6094 	Tidorum Ltd
PapaBench 	UAV autopilot and fly-by-wire software 	10 	6644 	Paparazzi
Powerwindow Distributed powerwindow control software  5 413
\end{verbatim}


\subsection{Issues with the Original Sources}


\todo{Show the issues with code snippets}

\begin{itemize}
\item Compiler optimization
\item byte order
\item low-level access to device registers
\item Some benchmarks, which are available for download in the Internet have a license that
does not allow open-source distribution. We removed those benchmarks from the original source.
\end{itemize}

\subsection{Benchmark Changes}



\begin{itemize}
\item unique names for functions and global variables
\item Sometimes movements of stack allocated variables into global
\item splitting of initialization and computation code into two functions
\item Loop bounds added
\end{itemize}


\subsection{Usage Recommendations}

\todo{Mmh shall we have this?}

\todo{can we include a paragraph about the taskset generator for schedulability analysis}

\begin{itemize}
\item Use a stable version. At the time of this writing we should have V 2.0
\item Do not edit the benchmarks
\item Use all benchmarks in your evaluation. Don't introduce a bias in your evaluation by
\emph{selecting the most representative benchmarks} for your improvement
\end{itemize}

\subsection{Licenses}

An issue we encountered with several benchmarks was that the original
source did not include any licensing information. In absence of such
information, we had to assume that the copyright holder reserves all
rights. Wherever necessary, we contacted the copyright holders to
obtain the right to use, modify, and redistribute the benchmarks. In a
small number of cases we however discovered that the code was in fact
under a license that made the benchmark unusable; the respective
benchmarks were consequently dropped from the TACLeBench benchmark
suite. All benchmarks in the benchmark suite now contain licensing
information, such that future developments do not require tracking
down the original authors of the benchmark.

\section{Evaluation}
\label{sec:eval}

\todo{Computer engineering is a constructive science. We build stuff and we measure.
Therefore, there shall always be an evaluation section.}
\todo{Properly discuss Figure~\ref{fig:execution-times}}
\begin{figure}[t]
  \def\resultfile{eval/wcet.csv}
  \input{eval/pasim-eval.tikz}
  \caption{Execution times in TACLeBench vary from \todo{x} to \todo{y} on the patmos architecture.}
  \label{fig:execution-times}
\end{figure}

\subsection{Sanity Checks}
All code-changes were done manually by different members of the TACLe Cost action.
Such a collaborative procedure is inherently error-prone. 
For once, human errors can occur and the original sources were often faulty to start with.
The distributive work on the benchmarks led to an additional source of errors: 
Different system configurations can lead to different behaviors, and a benchmark behaving well under system configuration A may be faulty under configuration B.
To ensure the quality of the benchmarks and to improve portability, we have thus implemented automatic sanity checks. 

The code quality and conformity with the code formatting rules is validated centrally and the latest results can be viewed online\footnote{\url{http://tacle.cs.fau.de/}}. 
For this sanity check, all benchmarks are compiled using \textit{gcc}, \textit{g$++$} and \textit{clang}, executed and the return values are checked against the expected value. 
Clang's static analyzer\footnote{\url{http://clang-analyzer.llvm.org/}} is used to further reveal programming bugs, such as out-of-bounds errors, and also to validate the compliance of the code with the code-formatting rules.

The portability has been checked via a shell-script \textit{checkbenchmarks.sh} that is now part of the TACLeBench git-repository.
The script allows to quickly identify incompatibilities of the benchmarks with specific operating systems, compilers and system configurations.
Even though full coverage of all system configurations can never be achieved, we were able to cover most of the common operating systems and compilers.

% In order to improve the code quality of the TACLeBench suite, we used different 
% compilers (e.g., clang, gcc) to identify shortcomings in the code of each benchmark in an 
% automated way. Additionally, we exploited Clang's static 
% analyzer\footnote{\url{http://clang-analyzer.llvm.org/}} to further reveal 
% programming bugs in the benchmark suite. We extended this sanity checker with additional 
% TACLeBench-specific passes to identify if all benchmark are compliant to the code-formatting rules.

\todo{Comparison compiler warning in TACLeBench v1 with v2, problem: expint was in 1.0, but is no 
longer in 2.0 and PowerWindow was added for 2.0}

Possible Options for the Evaluation:

\begin{itemize}
\item Compile for an embedded processor and measure execution time
\item Compile for Patmos and use pasim for execution cycles
\item Show how compiler optimizations optimized away the original code and how we fixed this
\item Complexity numbers with bar charts on execution time and maybe tools
\item Use some WCET tools
\end{itemize}


\section{Conclusion}
\label{sec:conclusion}

\todo{Rephrase what this paper is about and list the main contributions and results.}

\subparagraph*{Acknowledgements}

\todo{TACLe COST action phrase}

We want to thank \dots for helping in restructuring the benchmarks.
We want to thank Bendikt Huber for porting the Lift benchmark from Java to C.

\todo{Ack Niclas for contributing DEBIE in open-source.}

\appendix
\section{Source Access and Compiling the Benchmarks}


\bibliography{taclebench}


\end{document}
